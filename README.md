# GradientDescent_StudentData

This repository contains three core files, and a requirements.txt file.

The data was taken from http://www.ats.ucla.edu/stat/data/binary.csv

This exercise is concerned with implementing gradient descent into a neural network, i.e. the key element of the backwards propogation technique which serves to tweak the (initially random) weights of the activation function (from the forward pass). 

In doing so we seek to minimise the error by adjusting the weights of our prediction function such that we move down the negative of the gradient of the error function, and consequently improving the fit of our prediction function to the actual data.

The output is a trained network, and the accuracy is displayed when running the gradient.py file in your terminal.

For convenience I have uploaded my anaconda virtual environment setup (using python 3.6) so you can install and get started straight away.
